<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tutorial: Building a Containerized Data Lakehouse</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
    <style>
        /* Custom styles to enhance Tailwind */
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* slate-50 */
            color: #334155; /* slate-700 */
        }
        h1, h2, h3 {
            font-weight: 700;
            color: #0f172a; /* slate-900 */
        }
        pre {
            font-family: 'Fira Code', monospace;
            background-color: #1e293b; /* slate-800 */
            color: #e2e8f0; /* slate-200 */
            border-radius: 0.5rem;
            padding: 1.5rem;
            position: relative;
            overflow-x: auto;
        }
        .copy-btn {
            position: absolute;
            top: 0.75rem;
            right: 0.75rem;
            background-color: #475569; /* slate-600 */
            color: #cbd5e1; /* slate-300 */
            border: none;
            padding: 0.25rem 0.75rem;
            border-radius: 0.375rem;
            cursor: pointer;
            opacity: 0.7;
            transition: all 0.2s ease-in-out;
        }
        pre:hover .copy-btn {
            opacity: 1;
        }
        .copy-btn:hover {
            background-color: #64748b; /* slate-500 */
        }
        .copy-btn.copied {
            background-color: #16a34a; /* green-600 */
            color: white;
        }
        .nav-link {
            transition: all 0.2s ease-in-out;
            border-left: 2px solid transparent;
        }
        .nav-link.active {
            color: #2563eb; /* blue-600 */
            border-left-color: #2563eb; /* blue-600 */
            font-weight: 600;
        }
        .lesson-card {
            border-left: 4px solid;
            padding-left: 1.5rem;
        }
        .mistake {
            border-color: #dc2626; /* red-600 */
            background-color: #fef2f2; /* red-50 */
        }
        .lesson-learned {
            border-color: #16a34a; /* green-600 */
            background-color: #f0fdf4; /* green-50 */
        }
        /* Make content scrollable, not the whole body */
        .main-content {
            scroll-margin-top: 4rem;
        }
        .diagram-container {
            background-color: #ffffff;
            border: 1px solid #e2e8f0; /* slate-200 */
            border-radius: 0.75rem;
            padding: 1rem;
            margin-bottom: 3rem;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
            overflow-x: auto; /* This adds the horizontal scrollbar if content overflows */
        }
    </style>
</head>
<body class="antialiased">

    <div class="container mx-auto max-w-7xl px-4 sm:px-6 lg:px-8">
        <div class="lg:flex lg:space-x-12">
            <!-- Sticky Navigation Sidebar -->
            <aside class="hidden lg:block w-64 py-8 lg:py-12 flex-shrink-0">
                <div class="sticky top-8">
                    <h3 class="text-sm font-semibold uppercase tracking-wider text-slate-500">On this page</h3>
                    <nav id="side-nav" class="mt-4 flex flex-col space-y-2">
                        <a href="#goal" class="nav-link pl-4 py-1 text-slate-600 hover:text-blue-600">Goal</a>
                        <a href="#diagram" class="nav-link pl-4 py-1 text-slate-600 hover:text-blue-600">Architecture Diagram</a>
                        <a href="#phase1" class="nav-link pl-4 py-1 text-slate-600 hover:text-blue-600">Phase 1: Project Structure</a>
                        <a href="#phase2" class="nav-link pl-4 py-1 text-slate-600 hover:text-blue-600">Phase 2: The Dockerfile</a>
                        <a href="#phase3" class="nav-link pl-4 py-1 text-slate-600 hover:text-blue-600">Phase 3: Docker Compose</a>
                        <a href="#phase4" class="nav-link pl-4 py-1 text-slate-600 hover:text-blue-600">Phase 4: PySpark Code</a>
                        <a href="#lessons" class="nav-link pl-4 py-1 text-slate-600 hover:text-blue-600">Lessons Learned</a>
                    </nav>
                </div>
            </aside>

            <!-- Main Content -->
            <main class="flex-1 py-8 lg:py-12 min-w-0">
                <article class="prose max-w-none">
                    <section id="goal" class="main-content">
                        <p class="text-base font-semibold text-blue-600">Final Tutorial</p>
                        <h1 class="text-4xl lg:text-5xl tracking-tight">Building a Containerized Data Lakehouse</h1>
                        <p class="mt-6 text-xl text-slate-600">Package an entire data environment‚ÄîMinIO, PostgreSQL, Spark, and Jupyter‚Äîinto a portable, reproducible, and professional system using Docker. This tutorial provides the final, correct method and explains the critical lessons learned from common errors.</p>
                    </section>
                    
                    <section id="diagram" class="main-content mt-16">
                        <h2 class="text-2xl lg:text-3xl">Architecture Diagram</h2>
                        <div class="diagram-container">
                            <svg width="750" height="422" viewBox="0 0 800 450" xmlns="http://www.w3.org/2000/svg" font-family="'Inter', sans-serif">
                                <defs>
                                    <marker id="arrow" viewBox="0 0 10 10" refX="8" refY="5" markerWidth="6" markerHeight="6" orient="auto-start-reverse">
                                        <path d="M 0 0 L 10 5 L 0 10 z" fill="#64748b" />
                                    </marker>
                                </defs>

                                <!-- Docker Network Box -->
                                <rect x="150" y="50" width="630" height="370" rx="15" fill="#f1f5f9" stroke="#cbd5e1" stroke-width="2"/>
                                <text x="765" y="75" text-anchor="end" font-size="14" fill="#64748b" font-weight="600">Docker Network ("lakehouse-net")</text>

                                <!-- Components -->
                                <!-- User -->
                                <g transform="translate(40, 220)">
                                    <rect x="0" y="0" width="80" height="80" rx="10" fill="#bfdbfe" stroke="#60a5fa" stroke-width="2"/>
                                    <text x="40" y="35" text-anchor="middle" font-size="14" fill="#1e3a8a" font-weight="600">User</text>
                                    <text x="40" y="55" text-anchor="middle" font-size="24">üë®‚Äçüíª</text>
                                </g>

                                <!-- Spark/Jupyter -->
                                <g transform="translate(200, 100)">
                                    <rect x="0" y="0" width="220" height="120" rx="10" fill="#fefce8" stroke="#facc15" stroke-width="2"/>
                                    <text x="110" y="25" text-anchor="middle" font-size="14" fill="#854d0e" font-weight="600">spark-jupyter</text>
                                    <text x="15" y="55" font-size="12" fill="#4b5563">Jupyter Notebook</text>
                                    <text x="15" y="85" font-size="12" fill="#4b5563">PySpark Application</text>
                                    <circle cx="180" cy="70" r="25" fill="#f97316"/>
                                    <text x="180" y="75" text-anchor="middle" font-size="10" fill="white" font-weight="bold">Spark</text>
                                </g>

                                <!-- PostgreSQL -->
                                <g transform="translate(500, 100)">
                                    <rect x="0" y="0" width="220" height="120" rx="10" fill="#eef2ff" stroke="#818cf8" stroke-width="2"/>
                                    <text x="110" y="25" text-anchor="middle" font-size="14" fill="#3730a3" font-weight="600">postgres-catalog</text>
                                    <text x="15" y="60" font-size="12" fill="#4b5563">Iceberg Metadata</text>
                                    <text x="15" y="80" font-size="12" fill="#4b5563">(JDBC Catalog)</text>
                                    <path d="M165 50 h30 v40 h-30 z M165 70 h30 M180 50 v40" stroke="#4338ca" stroke-width="2" fill="none"/>
                                </g>

                                <!-- MinIO -->
                                <g transform="translate(350, 280)">
                                    <rect x="0" y="0" width="220" height="120" rx="10" fill="#f0fdf4" stroke="#4ade80" stroke-width="2"/>
                                    <text x="110" y="25" text-anchor="middle" font-size="14" fill="#15803d" font-weight="600">minio</text>
                                    <text x="15" y="60" font-size="12" fill="#4b5563">Data Warehouse</text>
                                    <text x="15" y="80" font-size="12" fill="#4b5563">(S3 Storage)</text>
                                    <path d="M177.5 50 A 15 15 0 0 1 177.5 80 M177.5 50 h-15 v30 h15" stroke="#16a34a" stroke-width="2" fill="none"/>
                                </g>

                                <!-- Arrows -->
                                <!-- User -> Jupyter -->
                                <line x1="120" y1="260" x2="190" y2="160" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)" />
                                <text x="155" y="220" font-size="11" fill="#475569">Writes PySpark Code</text>
                                <text x="155" y="235" font-size="11" fill="#475569">(Port 8888)</text>

                                <!-- Spark -> Postgres -->
                                <path d="M 420 160 C 460 160, 460 160, 500 160" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)" fill="none"/>
                                <text x="460" y="150" text-anchor="middle" font-size="11" fill="#475569">Manages Metadata (JDBC)</text>

                                <!-- Spark -> MinIO -->
                                <path d="M 310 220 C 310 250, 460 250, 460 280" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)" fill="none"/>
                                <text x="385" y="245" text-anchor="middle" font-size="11" fill="#475569">Reads/Writes Data (S3A)</text>
                            </svg>
                        </div>
                    </section>

                    <section id="phase1" class="main-content mt-16">
                        <h2 class="text-2xl lg:text-3xl">Phase 1: Project Structure</h2>
                        <p>A clean, self-contained project structure is essential for Docker. First, create the necessary directories for your project.</p>
                        <pre><code class="language-bash">mkdir -p ~/docker-lakehouse/spark
mkdir ~/docker-lakehouse/notebooks</code></pre>
                        <p class="mt-6">Next, pre-download all dependencies into the `spark` directory. This makes the Docker build robust and immune to network errors, as all files are part of the "build context".</p>
                        <pre><code class="language-bash"># Navigate to the spark directory
cd ~/docker-lakehouse/spark

# Download Spark and Hadoop
wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# Download all required JARs
wget https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5/1.5.2/iceberg-spark-runtime-3.5-1.5.2.jar
wget https://jdbc.postgresql.org/download/postgresql-42.7.3.jar
wget https://repo.maven.apache.org/maven2/software/amazon/awssdk/bundle/2.25.30/bundle-2.25.30.jar
wget https://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar</code></pre>
                    </section>

                    <section id="phase2" class="main-content mt-16">
                        <h2 class="text-2xl lg:text-3xl">Phase 2: The Dockerfile (The Recipe)</h2>
                        <p>This file, located at `~/docker-lakehouse/spark/Dockerfile`, is the recipe for building our custom Spark+Jupyter container.</p>
                        <pre><code class="language-dockerfile"># Start from a base Jupyter image
FROM jupyter/base-notebook:latest

# Switch to root to install software
USER root

# Install Java
RUN apt-get update && \
    apt-get install -y openjdk-17-jdk && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Copy archives into the image
COPY spark-3.5.1-bin-hadoop3.tgz /opt/
COPY hadoop-3.3.6.tar.gz /opt/

# Set all necessary environment variables
ENV SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
ENV HADOOP_HOME=/opt/hadoop-3.3.6
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HADOOP_HOME/bin
ENV LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH

# Extract archives
RUN tar -xzf /opt/spark-3.5.1-bin-hadoop3.tgz -C /opt/ && \
    tar -xzf /opt/hadoop-3.3.6.tar.gz -C /opt/ && \
    rm /opt/*.tgz

# Copy pre-downloaded JARs into Spark's jars directory
COPY *.jar $SPARK_HOME/jars/

# Install the py4j library for Python-to-Java communication
RUN pip install py4j

# Switch back to the non-root user
USER $NB_UID

# Set the working directory
WORKDIR /home/jovyan/work</code></pre>
                    </section>
                    
                    <section id="phase3" class="main-content mt-16">
                        <h2 class="text-2xl lg:text-3xl">Phase 3: The docker-compose.yml (The Conductor)</h2>
                        <p>This file, at `~/docker-lakehouse/docker-compose.yml`, defines and connects all our services: PostgreSQL for the catalog, MinIO for storage, and our custom Spark/Jupyter environment.</p>
                        <pre><code class="language-yaml">version: "3.8"
services:
  postgres-catalog:
    image: postgres:14
    container_name: postgres-catalog
    volumes:
      - postgres-data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=iceberg
      - POSTGRES_PASSWORD=iceberg
      - POSTGRES_DB=iceberg_catalog
    ports:
      - "5432:5432"
    networks:
      - lakehouse-net

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9090:9090"
    volumes:
      - minio-data:/data
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
      - MINIO_DEFAULT_BUCKETS=datalake
    command: server /data --console-address ":9090"
    networks:
      - lakehouse-net

  spark-jupyter:
    build: ./spark
    container_name: spark-jupyter
    ports:
      - "8888:8888"
      - "4040:4040"
    volumes:
      - ./notebooks:/home/jovyan/work
    networks:
      - lakehouse-net
volumes:
  postgres-data:
  minio-data:
networks:
  lakehouse-net:
    driver: bridge</code></pre>
                    </section>

                    <section id="phase4" class="main-content mt-16">
                        <h2 class="text-2xl lg:text-3xl">Phase 4: The PySpark Notebook Code (The Application)</h2>
                        <p>This is the final, correct code to run in the first cell of every new Jupyter notebook. It uses Spark's built-in package manager to reliably configure all dependencies.</p>
                        <pre><code class="language-python">from pyspark.sql import SparkSession

# This is the final and most reliable configuration.
spark = SparkSession.builder \\
    .appName("DockerIcebergFinal") \\
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.postgresql:postgresql:42.7.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \\
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \\
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \\
    .config("spark.sql.catalog.local.type", "jdbc") \\
    .config("spark.sql.catalog.local.uri", "jdbc:postgresql://postgres-catalog:5432/iceberg_catalog") \\
    .config("spark.sql.catalog.local.jdbc.user", "iceberg") \\
    .config("spark.sql.catalog.local.jdbc.password", "iceberg") \\
    .config("spark.sql.catalog.local.warehouse", "s3a://datalake/warehouse") \\
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \\
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \\
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \\
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \\
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \\
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \\
    .getOrCreate()

print("SparkSession created successfully!")</code></pre>
                    </section>

                    <section id="lessons" class="main-content mt-16">
                        <h2 class="text-2xl lg:text-3xl">Lessons Learned: A Summary of Our Battle</h2>
                        <p>The journey to containerize this project was difficult but taught several critical lessons about Spark and Docker.</p>
                        <div class="space-y-8 mt-8">
                            <div class="lesson-card mistake p-6 rounded-lg">
                                <h3 class="text-lg font-semibold text-red-800">Mistake 1: Managing JARs Manually</h3>
                                <p class="mt-2 text-red-700">Our biggest struggle was trying to avoid Spark's package manager to speed up startup. We tried copying JARs into the Dockerfile and setting `spark.jars` in the Python code. This led to a series of `ClassNotFoundException` errors.</p>
                                <div class="lesson-learned mt-4 p-4 rounded-lg">
                                    <p class="font-semibold text-green-800">üí° Lesson Learned:</p>
                                    <p class="mt-1 text-green-700">For complex dependencies like `hadoop-aws`, Spark's own package manager (`spark.jars.packages`) is the most reliable method. It correctly resolves the entire tree of sub-dependencies. The one-time download cost is worth the stability.</p>
                                </div>
                            </div>
                            <div class="lesson-card mistake p-6 rounded-lg">
                                <h3 class="text-lg font-semibold text-red-800">Mistake 2: Using `findspark` in Docker</h3>
                                <p class="mt-2 text-red-700">We initially used `findspark` to bridge our local Python environment with a manual Spark installation. When we moved to Docker, this became an anti-pattern.</p>
                                <div class="lesson-learned mt-4 p-4 rounded-lg">
                                    <p class="font-semibold text-green-800">üí° Lesson Learned:</p>
                                    <p class="mt-1 text-green-700">`findspark` is a tool for messy, local setups. In a clean, containerized environment built with a Dockerfile, the environment variables (`SPARK_HOME`, `PYTHONPATH`) are the correct way to make PySpark available.</p>
                                </div>
                            </div>
                            <div class="lesson-card mistake p-6 rounded-lg">
                                <h3 class="text-lg font-semibold text-red-800">Mistake 3: Forgetting Python-level Dependencies</h3>
                                <p class="mt-2 text-red-700">We correctly installed Spark in the Dockerfile, but our notebook failed with `ModuleNotFoundError: No module named 'py4j'`.</p>
                                <div class="lesson-learned mt-4 p-4 rounded-lg">
                                    <p class="font-semibold text-green-800">üí° Lesson Learned:</p>
                                    <p class="mt-1 text-green-700">A Docker image is a completely isolated system. You must explicitly install every dependency the application needs, including system packages (`openjdk-17-jdk`) and Python packages (`pip install py4j`).</p>
                                </div>
                            </div>
                             <div class="lesson-card mistake p-6 rounded-lg">
                                <h3 class="text-lg font-semibold text-red-800">Mistake 4: Re-using a Broken SparkSession</h3>
                                <p class="mt-2 text-red-700">We frequently saw an error return after a restart. This was caused by the warning: "Using an existing Spark session."</p>
                                <div class="lesson-learned mt-4 p-4 rounded-lg">
                                    <p class="font-semibold text-green-800">üí° Lesson Learned:</p>
                                    <p class="mt-1 text-green-700">A SparkSession is a persistent object. If created incorrectly, it can remain in a broken state. `getOrCreate()` will keep returning this broken session. The solution is to always use <strong>Kernel > Restart Kernel</strong> in Jupyter to ensure a clean slate.</p>
                                </div>
                            </div>
                        </div>
                    </section>
                    
                    <footer class="mt-16 text-center text-slate-500">
                        <p>Congratulations on completing this challenging and realistic project!</p>
                    </footer>
                </article>
            </main>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            // --- Copy to Clipboard Functionality ---
            const allCodeBlocks = document.querySelectorAll('pre');

            allCodeBlocks.forEach(pre => {
                const code = pre.querySelector('code');
                if (code) {
                    const copyButton = document.createElement('button');
                    copyButton.className = 'copy-btn';
                    copyButton.textContent = 'Copy';
                    pre.appendChild(copyButton);

                    copyButton.addEventListener('click', () => {
                        const textToCopy = code.innerText;
                        // Using document.execCommand for broader compatibility in sandboxed environments
                        const textArea = document.createElement('textarea');
                        textArea.value = textToCopy;
                        document.body.appendChild(textArea);
                        textArea.select();
                        try {
                            document.execCommand('copy');
                            copyButton.textContent = 'Copied!';
                            copyButton.classList.add('copied');
                        } catch (err) {
                            console.error('Failed to copy text: ', err);
                            copyButton.textContent = 'Error';
                        }
                        document.body.removeChild(textArea);

                        setTimeout(() => {
                            copyButton.textContent = 'Copy';
                            copyButton.classList.remove('copied');
                        }, 2000);
                    });
                }
            });

            // --- Active Nav Link on Scroll ---
            const sections = document.querySelectorAll('.main-content');
            const navLinks = document.querySelectorAll('#side-nav a');

            const observerOptions = {
                root: null, // viewport
                rootMargin: '0px',
                threshold: 0.3 // 30% of the section must be visible
            };

            const observer = new IntersectionObserver((entries, obs) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        const id = entry.target.getAttribute('id');
                        navLinks.forEach(link => {
                            link.classList.remove('active');
                            if (link.getAttribute('href') === `#${id}`) {
                                link.classList.add('active');
                            }
                        });
                    }
                });
            }, observerOptions);

            sections.forEach(section => {
                observer.observe(section);
            });
            
            // --- Smooth scrolling for nav links ---
            navLinks.forEach(anchor => {
                anchor.addEventListener('click', function (e) {
                    e.preventDefault();
                    document.querySelector(this.getAttribute('href')).scrollIntoView({
                        behavior: 'smooth'
                    });
                });
            });
        });
    </script>

</body>
</html>
