<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Data Lakehouse Tutorial</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; }
        .code-block, .terminal { font-family: 'Fira Code', monospace; }
        .nav-link { transition: all 0.2s ease-in-out; }
        .nav-link.active {
            background-color: #0284c7; /* sky-600 */
            color: white;
            font-weight: 600;
        }
        .nav-link:not(.active):hover {
            background-color: #f1f5f9; /* slate-100 */
            color: #0f172a; /* slate-900 */
        }
        .content-section { display: none; }
        .content-section.active { display: block; }

        /* --- SVG Diagram Styles --- */
        .svg-diagram .component-card {
            cursor: pointer;
            transition: all 0.2s ease-in-out;
        }
        /* Define the drop shadow filter */
        .svg-diagram .component-card .card-rect {
            transition: all 0.2s ease-in-out;
            filter: drop-shadow(0 4px 6px rgb(0 0 0 / 0.05));
        }
        /* Hover effect for the SVG cards */
        .svg-diagram .component-card:hover .card-rect {
            transform: translateY(-4px);
            filter: drop-shadow(0 10px 15px rgb(0 0 0 / 0.1));
        }
        .svg-diagram .connector {
            stroke: #94a3b8; /* slate-400 */
            stroke-width: 2;
            fill: none;
        }
        .svg-diagram .card-title {
            font-size: 16px;
            font-weight: 700;
            fill: #1e293b; /* slate-800 */
        }
        .svg-diagram .card-subtitle {
            font-size: 13px;
            fill: #475569; /* slate-600 */
        }
    </style>
</head>
<body class="bg-slate-50 text-slate-800">

    <div class="flex flex-col md:flex-row min-h-screen">
        <!-- Sidebar Navigation -->
        <aside class="w-full md:w-64 bg-white border-r border-slate-200 p-4 md:p-6 space-y-4">
            <h1 class="text-xl font-bold text-sky-700">Lakehouse Tutorial</h1>
            <nav class="space-y-2">
                <a href="#architecture" class="nav-link flex items-center p-3 rounded-lg active">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-3" viewBox="0 0 20 20" fill="currentColor"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z" /></svg>
                    Architecture
                </a>
                <a href="#setup" class="nav-link flex items-center p-3 rounded-lg">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-3" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm1-11a1 1 0 10-2 0v2H7a1 1 0 100 2h2v2a1 1 0 102 0v-2h2a1 1 0 100-2h-2V7z" clip-rule="evenodd" /></svg>
                    Setup Guide
                </a>
                <a href="#workflow" class="nav-link flex items-center p-3 rounded-lg">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-3" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M10 2a1 1 0 00-1 1v1a1 1 0 002 0V3a1 1 0 00-1-1zM4 4a1 1 0 00-1 1v1a1 1 0 002 0V5a1 1 0 00-1-1zm12 0a1 1 0 00-1 1v1a1 1 0 002 0V5a1 1 0 00-1-1zM4 10a1 1 0 00-1 1v1a1 1 0 002 0v-1a1 1 0 00-1-1zm12 0a1 1 0 00-1 1v1a1 1 0 002 0v-1a1 1 0 00-1-1zM10 16a1 1 0 00-1 1v1a1 1 0 002 0v-1a1 1 0 00-1-1z" clip-rule="evenodd" /><path d="M3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" /></svg>
                    Workflow & Scripts
                </a>
                <a href="#demo" class="nav-link flex items-center p-3 rounded-lg">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-3" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M2 5a2 2 0 012-2h12a2 2 0 012 2v10a2 2 0 01-2 2H4a2 2 0 01-2-2V5zm3.293 1.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" /></svg>
                    Interactive Demo
                </a>
                <a href="#jupyter-pyspark" class="nav-link flex items-center p-3 rounded-lg">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-3" viewBox="0 0 20 20" fill="currentColor"><path d="M11 3a1 1 0 100 2h2.586l-6.293 6.293a1 1 0 101.414 1.414L15 6.414V9a1 1 0 102 0V4a1 1 0 00-1-1h-5z" /><path d="M5 5a2 2 0 00-2 2v8a2 2 0 002 2h8a2 2 0 002-2v-3a1 1 0 10-2 0v3H5V7h3a1 1 0 000-2H5z" /></svg>
                    Jupyter & PySpark
                </a>
            </nav>
        </aside>

        <!-- Main Content -->
        <main class="flex-1 p-6 md:p-10 overflow-y-auto">
            <!-- Architecture Section -->
            <section id="architecture" class="content-section active">
                <h2 class="text-3xl font-bold mb-4">Lakehouse Architecture</h2>
                <p class="mb-8 text-lg text-slate-600">A data lakehouse combines the best of data lakes and data warehouses. Our project uses four key components that work together in a clear data flow. This diagram shows the flow when using the `spark-sql` command-line interface. Click on any component to learn its role.</p>
                
                <!-- SVG Diagram Start -->
                <div class="w-full max-w-2xl mx-auto">
                    <svg class="svg-diagram" width="100%" viewBox="0 0 500 520" xmlns="http://www.w3.org/2000/svg">
                        <!-- Defs for arrowhead marker. refX="10" makes the arrow tip align with the path's end vertex. -->
                        <defs>
                            <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#94a3b8" />
                            </marker>
                        </defs>

                        <!-- Components (drawn first, so they are in the background) -->
                        <text x="250" y="25" text-anchor="middle" class="card-title" style="font-size: 18px;">üë®‚Äçüíª You (The User)</text>

                        <g id="card-spark" class="component-card">
                            <rect class="card-rect" x="100" y="90" width="300" height="80" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="250" y="125" text-anchor="middle" class="card-title">üöÄ Apache Spark</text>
                            <text x="250" y="150" text-anchor="middle" class="card-subtitle">The Query Engine</text>
                        </g>

                        <g id="card-iceberg" class="component-card">
                            <rect class="card-rect" x="100" y="225" width="300" height="80" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="250" y="260" text-anchor="middle" class="card-title">üßä Apache Iceberg</text>
                            <text x="250" y="285" text-anchor="middle" class="card-subtitle">The Table Manager</text>
                        </g>

                        <g id="card-postgres" class="component-card">
                            <rect class="card-rect" x="50" y="400" width="200" height="90" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="150" y="435" text-anchor="middle" class="card-title">üêò PostgreSQL</text>
                            <text x="150" y="460" text-anchor="middle" class="card-subtitle">The Catalog</text>
                            <text x="150" y="475" text-anchor="middle" class="card-subtitle">(Address Book)</text>
                        </g>

                        <g id="card-minio" class="component-card">
                            <rect class="card-rect" x="250" y="400" width="200" height="90" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="350" y="435" text-anchor="middle" class="card-title">üì¶ MinIO</text>
                            <text x="350" y="460" text-anchor="middle" class="card-subtitle">The Storage Layer</text>
                        </g>

                        <!-- Connectors (drawn last, so they appear on top). Paths go from edge to edge. -->
                        <path class="connector" d="M 250 40 V 90" marker-end="url(#arrowhead)"></path>
                        <path class="connector" d="M 250 170 V 225" marker-end="url(#arrowhead)"></path>
                        <path class="connector" d="M 250 305 V 350 H 150 V 400" marker-end="url(#arrowhead)"></path>
                        <path class="connector" d="M 250 305 V 350 H 350 V 400" marker-end="url(#arrowhead)"></path>
                    </svg>
                </div>
                <!-- SVG Diagram End -->
                <div id="component-description" class="mt-8 p-6 bg-sky-50 border border-sky-200 rounded-xl text-lg text-slate-700">Select a component above to see its description.</div>
            </section>

            <section id="setup" class="content-section">
                 <h2 class="text-3xl font-bold mb-4">Setup Guide</h2>
                <p class="mb-8 text-lg text-slate-600">Goal: To add a structured, transactional table layer (Apache Iceberg) on top of our MinIO object storage. This turns our basic data lake into a powerful, modern data lakehouse.</p>
                <div class="space-y-6">
                    <div>
                        <h3 class="text-xl font-semibold mb-2">1. Prerequisites: Java Environment</h3>
                        <p class="mb-2">Apache Spark requires a Java Development Kit (JDK). If you haven't installed it, run the following command:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>sudo apt update && sudo apt install openjdk-17-jdk -y

# Set the JAVA_HOME variable so Spark can find it
echo 'export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64' >> ~/.bashrc
source ~/.bashrc</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                    </div>
                    <div>
                        <h3 class="text-xl font-semibold mb-2">2. The Iceberg Catalog: PostgreSQL via Docker</h3>
                        <p class="mb-2">Iceberg needs a "catalog" to store table metadata (schemas, history, etc.). We'll use a PostgreSQL database running in Docker for this.</p>
                        <p class="mb-2 font-medium">Install Docker:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code># Install Docker and Docker Compose
sudo apt install docker.io docker-compose -y
sudo usermod -aG docker ${USER}
newgrp docker</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        <p class="my-2 font-medium">Create docker-compose.yml:</p>
                        <p class="mb-2">In your <code>~/minio</code> directory, create a file named <code>docker-compose.yml</code> with the following content:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>services:
  postgres-catalog:
    image: postgres:14
    container_name: postgres-catalog
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=iceberg
      - POSTGRES_PASSWORD=iceberg
      - POSTGRES_DB=iceberg_catalog
    volumes:
      - postgres-catalog-data:/var/lib/postgresql/data

volumes:
  postgres-catalog-data:</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                    </div>
                    <div>
                        <h3 class="text-xl font-semibold mb-2">3. The Query Engine: Apache Spark</h3>
                        <p class="mb-2">Spark will be the engine that runs our SQL queries. First, navigate to your project directory:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>cd ~/minio</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        <p class="my-2">Download and Extract Spark:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
tar -xzf spark-3.5.1-bin-hadoop3.tgz</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        <p class="mt-2 text-sm italic">Note: We will not download any dependencies (JAR files) manually. We will let Spark handle this automatically, which is a more robust method.</p>
                    </div>
                    <div>
                        <h3 class="text-xl font-semibold mb-2">4. Configure MinIO Security for Spark (CRITICAL STEP)</h3>
                        <p class="mb-2">Before using Spark, we must create a dedicated user for it in MinIO. This is a critical security practice.</p>
                        <p class="my-2 font-medium">Connect mc to your MinIO server:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code># Run this from ~/minio, assuming your server is running
./mc alias set local http://127.0.0.1:9000 minioadmin minioadmin</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        <p class="my-2 font-medium">Create the datalake bucket:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>./mc mb local/datalake</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        <p class="my-2 font-medium">Create a dedicated user for Spark:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>./mc admin user add local spark-user spark-password-123</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        <p class="my-2 font-medium">Create a policy file (datalake-policy.json):</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>echo '{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": ["s3:*"],
            "Resource": ["arn:aws:s3:::datalake/*"]
        }
    ]
}' > datalake-policy.json</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        <p class="my-2 font-medium">Add and attach the policy:</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>./mc admin policy create local datalake-policy datalake-policy.json
./mc admin policy attach local datalake-policy --user spark-user</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                    </div>
                </div>
            </section>

            <section id="workflow" class="content-section">
                <h2 class="text-3xl font-bold mb-4">Workflow & Scripts</h2>
                <p class="mb-8 text-lg text-slate-600">Use these scripts to easily start and stop your lakehouse environment. Place them in your main project folder (`~/minio`).</p>
                <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                    <div>
                        <h3 class="text-xl font-semibold mb-2">Backend Startup Script</h3>
                        <p class="mb-2">Save as `start_lakehouse.sh`. This starts MinIO and PostgreSQL.</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>#!/bin/bash
  
echo "--- Starting MinIO Server ---"
./minio server ~/minio-data --console-address ":9090" > minio.log 2>&1 &

echo "--- Starting PostgreSQL Catalog ---"
docker compose up -d

echo ""
echo "Lakehouse foundation is ready."</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                    </div>
                    <div>
                        <h3 class="text-xl font-semibold mb-2">Spark Launch Script</h3>
                        <p class="mb-2">Save as `start_spark.sh`. This starts the Spark SQL shell.</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative h-full"><pre class="code-block"><code>#!/bin/bash
  
SPARK_HOME="$HOME/minio/spark-3.5.1-bin-hadoop3"

"$SPARK_HOME/bin/spark-sql" \
--packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.postgresql:postgresql:42.7.3,software.amazon.awssdk:bundle:2.25.30,org.apache.hadoop:hadoop-aws:3.3.4 \
--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
--conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
--conf spark.sql.catalog.local.type=jdbc \
--conf spark.sql.catalog.local.uri=jdbc:postgresql://localhost:5432/iceberg_catalog \
--conf spark.sql.catalog.local.jdbc.user=iceberg \
--conf spark.sql.catalog.local.jdbc.password=iceberg \
--conf spark.sql.catalog.local.jdbc.schema-version=V1 \
--conf spark.sql.catalog.local.warehouse=s3a://datalake/warehouse \
--conf spark.hadoop.fs.s3a.endpoint=http://127.0.0.1:9000 \
--conf spark.hadoop.fs.s3a.access.key=spark-user \
--conf spark.hadoop.fs.s3a.secret.key=spark-password-123 \
--conf spark.hadoop.fs.s3a.path.style.access=true \
--conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                    </div>
                </div>
            </section>

            <section id="demo" class="content-section">
                <h2 class="text-3xl font-bold mb-4">Interactive Demo</h2>
                <p class="mb-8 text-lg text-slate-600">Click the "Run Next Command" button to step through the process of creating a table, evolving its schema, and traveling back in time to view old data.</p>
                <div class="bg-slate-900 rounded-xl shadow-lg">
                    <div class="bg-slate-800 rounded-t-xl p-2 flex items-center"><div class="w-3 h-3 bg-red-500 rounded-full mr-2"></div><div class="w-3 h-3 bg-yellow-500 rounded-full mr-2"></div><div class="w-3 h-3 bg-green-500 rounded-full"></div><span class="text-slate-400 text-sm ml-4">spark-sql-demo</span></div>
                    <div id="terminal" class="terminal p-4 h-96 overflow-y-auto text-white text-sm"><div class="text-green-400">Welcome to the interactive Iceberg demo!</div></div>
                    <div class="p-4 border-t border-slate-700 text-center"><button id="run-demo-btn" class="bg-sky-600 hover:bg-sky-500 text-white font-bold py-2 px-6 rounded-lg transition-colors">Run Next Command</button></div>
                </div>
            </section>

            <section id="jupyter-pyspark" class="content-section">
                <h2 class="text-3xl font-bold mb-4">Connecting JupyterLab to PySpark</h2>
                <p class="mb-8 text-lg text-slate-600">Goal: To set up a professional, stable, and interactive development environment for PySpark using JupyterLab, connected to our existing data lakehouse.</p>
                
                <h3 class="text-2xl font-semibold mb-4">Jupyter Workflow Architecture</h3>
                <p class="mb-8 text-lg text-slate-600">This diagram shows how Jupyter and PySpark fit into our lakehouse stack. You write Python code in a notebook, and the PySpark library translates it into Spark jobs that run on the same powerful backend.</p>
                
                <!-- New SVG Diagram for Jupyter -->
                <div class="w-full max-w-2xl mx-auto my-8">
                    <svg class="svg-diagram" width="100%" viewBox="0 0 500 650" xmlns="http://www.w3.org/2000/svg">
                        <defs>
                            <marker id="arrowhead-jupyter" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#94a3b8" />
                            </marker>
                        </defs>
                        <g id="card-jupyter" class="component-card">
                            <rect class="card-rect" x="100" y="10" width="300" height="80" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="250" y="45" text-anchor="middle" class="card-title">üíª JupyterLab</text>
                            <text x="250" y="70" text-anchor="middle" class="card-subtitle">Your Python IDE</text>
                        </g>
                        <g id="card-pyspark" class="component-card">
                            <rect class="card-rect" x="100" y="140" width="300" height="80" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="250" y="175" text-anchor="middle" class="card-title">üêç PySpark Session</text>
                            <text x="250" y="200" text-anchor="middle" class="card-subtitle">Connects Python to Spark</text>
                        </g>
                        <g id="card-spark-2" class="component-card">
                            <rect class="card-rect" x="100" y="270" width="300" height="80" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="250" y="305" text-anchor="middle" class="card-title">üöÄ Apache Spark</text>
                            <text x="250" y="330" text-anchor="middle" class="card-subtitle">The Query Engine</text>
                        </g>
                        <g id="card-iceberg-2" class="component-card">
                            <rect class="card-rect" x="100" y="400" width="300" height="80" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="250" y="435" text-anchor="middle" class="card-title">üßä Apache Iceberg</text>
                            <text x="250" y="460" text-anchor="middle" class="card-subtitle">The Table Manager</text>
                        </g>
                        <g id="card-postgres-2" class="component-card">
                            <rect class="card-rect" x="50" y="540" width="200" height="90" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="150" y="575" text-anchor="middle" class="card-title">üêò PostgreSQL</text>
                            <text x="150" y="600" text-anchor="middle" class="card-subtitle">The Catalog</text>
                        </g>
                        <g id="card-minio-2" class="component-card">
                            <rect class="card-rect" x="250" y="540" width="200" height="90" rx="12" fill="white" stroke="#e2e8f0"></rect>
                            <text x="350" y="575" text-anchor="middle" class="card-title">üì¶ MinIO</text>
                            <text x="350" y="600" text-anchor="middle" class="card-subtitle">The Storage Layer</text>
                        </g>
                        <path class="connector" d="M 250 90 V 140" marker-end="url(#arrowhead-jupyter)"></path>
                        <path class="connector" d="M 250 220 V 270" marker-end="url(#arrowhead-jupyter)"></path>
                        <path class="connector" d="M 250 350 V 400" marker-end="url(#arrowhead-jupyter)"></path>
                        <path class="connector" d="M 250 480 V 500 H 150 V 540" marker-end="url(#arrowhead-jupyter)"></path>
                        <path class="connector" d="M 250 480 V 500 H 350 V 540" marker-end="url(#arrowhead-jupyter)"></path>
                    </svg>
                </div>

                <div class="space-y-8">
                    <div>
                        <h3 class="text-2xl font-semibold mb-4 border-b pb-2">1. Installation</h3>
                        <div class="space-y-4">
                            <p>Activate your Conda environment (if you use one):</p>
                            <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>conda activate watcher-env</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                            <p>Install JupyterLab and `findspark`, a crucial helper library:</p>
                            <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>pip install jupyterlab findspark</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                            <p class="font-bold text-red-600">CRITICAL: Uninstall any conflicting PySpark library to avoid version conflicts.</p>
                            <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>pip uninstall pyspark</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                            <p>(Optional) Download Native Hadoop Libraries to fix startup warnings:</p>
                            <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code># Navigate to your main project directory
cd ~/minio

# Download and extract the matching Hadoop version
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar -xzf hadoop-3.3.6.tar.gz</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        </div>
                    </div>
                    <div>
                        <h3 class="text-2xl font-semibold mb-4 border-b pb-2">2. The Startup Scripts</h3>
                        <div class="space-y-4">
                             <h4 class="text-xl font-semibold mb-2">2.1. Backend Script (start_lakehouse.sh)</h4>
                             <p>This script (located in <code>~/minio</code>) starts MinIO and PostgreSQL. It remains unchanged.</p>
                             <h4 class="text-xl font-semibold mb-2 mt-4">2.2. Jupyter Launch Script (start_jupyter.sh)</h4>
                             <p>Save this as <code>start_jupyter.sh</code> in your <code>~/minio</code> folder. It prepares environment variables and starts JupyterLab.</p>
                             <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative"><pre class="code-block"><code>#!/bin/bash

# Define the location of your Spark installation
export SPARK_HOME="$HOME/minio/spark-3.5.1-bin-hadoop3"

# Define the location of your Hadoop installation to fix the native library warning
export HADOOP_HOME="$HOME/minio/hadoop-3.3.6"

# Add Hadoop's native libraries to the system's library path
export LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH"

# --- Launch JupyterLab ---
echo "Starting JupyterLab..."
jupyter lab --ip=0.0.0.0 --notebook-dir=$HOME/minio</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                        </div>
                    </div>
                    <div>
                        <h3 class="text-2xl font-semibold mb-4 border-b pb-2">3. Jupyter Notebook Connection Code</h3>
                        <p>Run this code in the first cell of your notebook every time you start or restart the kernel.</p>
                        <div class="code-container bg-slate-800 rounded-lg p-4 text-white relative mt-4"><pre class="code-block"><code># 1. First, find Spark and add it to the environment. This MUST be run before importing pyspark.
import findspark
findspark.init('/home/ubuntu/minio/spark-3.5.1-bin-hadoop3')

# 2. Now you can import pyspark successfully
from pyspark.sql import SparkSession

# 3. Build the SparkSession with the full, correct configuration
spark = SparkSession.builder \
    .appName("IcebergJupyter") \
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.postgresql:postgresql:42.7.3,software.amazon.awssdk:bundle:2.25.30,org.apache.hadoop:hadoop-aws:3.3.4") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.local.type", "jdbc") \
    .config("spark.sql.catalog.local.uri", "jdbc:postgresql://localhost:5432/iceberg_catalog") \
    .config("spark.sql.catalog.local.jdbc.user", "iceberg") \
    .config("spark.sql.catalog.local.jdbc.password", "iceberg") \
    .config("spark.sql.catalog.local.jdbc.schema-version", "V1") \
    .config("spark.sql.catalog.local.warehouse", "s3a://datalake/warehouse") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://127.0.0.1:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "spark-user") \
    .config("spark.hadoop.fs.s3a.secret.key", "spark-password-123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

print("SparkSession created successfully!")</code></pre><button class="copy-btn absolute top-3 right-3 bg-slate-600 hover:bg-slate-500 text-white text-xs font-semibold py-1 px-2 rounded">Copy</button></div>
                    </div>
                     <div>
                        <h3 class="text-2xl font-semibold mb-4 border-b pb-2">4. Troubleshooting Summary</h3>
                        <div class="space-y-4">
                            <div class="p-4 bg-white rounded-lg border">
                                <h4 class="font-semibold text-lg">Problem 1: `TypeError: 'JavaPackage' object is not callable`</h4>
                                <p class="mt-1"><strong class="text-slate-600">Reason:</strong> A version conflict between a `pyspark` library in your Python environment and the main Spark engine.</p>
                                <p class="mt-1"><strong class="text-sky-700">Solution:</strong> The definitive fix is to uninstall PySpark from the Python environment (`pip uninstall pyspark`) and use `findspark` to connect to the main Spark engine.</p>
                            </div>
                            <div class="p-4 bg-white rounded-lg border">
                                <h4 class="font-semibold text-lg">Problem 2: `ModuleNotFoundError: No module named 'pyspark'`</h4>
                                <p class="mt-1"><strong class="text-slate-600">Reason:</strong> Trying to `import pyspark` before `findspark.init()` has told Python where to find the module.</p>
                                <p class="mt-1"><strong class="text-sky-700">Solution:</strong> Ensure `findspark.init()` is the very first Spark-related command you run in your notebook.</p>
                            </div>
                            <div class="p-4 bg-white rounded-lg border">
                                <h4 class="font-semibold text-lg">Problem 3: `Exception: Unable to find py4j...`</h4>
                                <p class="mt-1"><strong class="text-slate-600">Reason:</strong> A typo in the path provided to `findspark.init()`.</p>
                                <p class="mt-1"><strong class="text-sky-700">Solution:</strong> Double-check and correct the path to your Spark home directory.</p>
                            </div>
                             <div class="p-4 bg-white rounded-lg border">
                                <h4 class="font-semibold text-lg">Problem 4: `Py4JJavaError` when running a `spark.sql()` command</h4>
                                <p class="mt-1"><strong class="text-slate-600">Reason:</strong> A typo in a configuration value, such as an incorrect port in the PostgreSQL connection string.</p>
                                <p class="mt-1"><strong class="text-sky-700">Solution:</strong> Carefully verify all configuration strings, like the catalog URI, for typos.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </main>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const navLinks = document.querySelectorAll('.nav-link');
            const contentSections = document.querySelectorAll('.content-section');

            function switchTab(hash) {
                navLinks.forEach(link => {
                    link.classList.toggle('active', link.getAttribute('href') === hash);
                });
                contentSections.forEach(section => {
                    section.classList.toggle('active', `#${section.id}` === hash);
                });
            }
            
            navLinks.forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = link.getAttribute('href');
                    window.location.hash = targetId;
                });
            });

            window.addEventListener('hashchange', () => {
                const hash = window.location.hash || '#architecture';
                switchTab(hash);
            });

            // Set initial tab based on URL hash
            const initialHash = window.location.hash || '#architecture';
            switchTab(initialHash);

            // --- Copy Button Functionality ---
            document.querySelectorAll('.copy-btn').forEach(button => {
                button.addEventListener('click', () => {
                    const code = button.closest('.code-container').querySelector('pre.code-block').innerText;
                    const textarea = document.createElement('textarea');
                    textarea.value = code;
                    document.body.appendChild(textarea);
                    textarea.select();
                    try {
                        document.execCommand('copy');
                        button.textContent = 'Copied!';
                        setTimeout(() => { button.textContent = 'Copy'; }, 2000);
                    } catch (err) {
                        console.error('Failed to copy text: ', err);
                    }
                    document.body.removeChild(textarea);
                });
            });

            // --- Interactive Architecture Diagram ---
            const componentCards = document.querySelectorAll('.component-card');
            const descriptionBox = document.getElementById('component-description');
            const descriptions = {
                'card-spark': "<strong>üöÄ Apache Spark (The Engine):</strong> You write standard SQL to Spark. Its mission is to understand your SQL, plan the most efficient execution, and perform the computation.",
                'card-iceberg': "<strong>üßä Apache Iceberg (The Table Manager):</strong> Spark delegates table operations to Iceberg. Its mission is to handle the complex logic of transactions, schema changes, and time travel, creating metadata files that act as a 'blueprint' for the table.",
                'card-postgres': "<strong>üêò PostgreSQL (The Catalog):</strong> Iceberg needs to know where the latest 'blueprint' for a table is. It asks PostgreSQL, which acts as an 'address book,' storing only the path to the current metadata file for each table. This is an atomic operation, preventing corruption.",
                'card-minio': "<strong>üì¶ MinIO (The Storage Layer):</strong> Once Iceberg has the blueprint from the catalog, it knows which data files to read or write. It interacts with MinIO, whose mission is to simply store and retrieve the physical data and metadata files."
            };

            componentCards.forEach(card => {
                card.addEventListener('click', () => {
                    descriptionBox.innerHTML = descriptions[card.id] || "Select a component to see its description.";
                });
            });

            // --- Interactive Demo Terminal ---
            const terminal = document.getElementById('terminal');
            const runDemoBtn = document.getElementById('run-demo-btn');
            let demoStep = 0;

            const demoScript = [
                { type: 'command', text: 'CREATE DATABASE local.db;' },
                { type: 'response', text: 'OK<br>Time taken: 0.2s' },
                { type: 'command', text: 'USE local.db;' },
                { type: 'response', text: 'OK<br>Time taken: 0.1s' },
                { type: 'command', text: 'CREATE TABLE my_first_table (id BIGINT, data STRING) USING iceberg;' },
                { type: 'response', text: 'OK<br>Time taken: 0.8s' },
                { type: 'command', text: "INSERT INTO my_first_table VALUES (1, 'hello world'), (2, 'hello iceberg');" },
                { type: 'response', text: 'OK<br>Time taken: 1.1s' },
                { type: 'info', text: '--- Now, let\'s demonstrate Schema Evolution ---' },
                { type: 'command', text: 'ALTER TABLE my_first_table ADD COLUMN event_timestamp TIMESTAMP;' },
                { type: 'response', text: 'OK<br>Time taken: 0.3s' },
                { type: 'command', text: 'SELECT * FROM my_first_table;' },
                { type: 'response', text: '<pre class="text-xs whitespace-pre-wrap">+---+-------------+-----------------+\n|id |data         |event_timestamp  |\n+---+-------------+-----------------+\n|1  |hello world  |null             |\n|2  |hello iceberg|null             |\n+---+-------------+-----------------+</pre>Fetched 2 row(s)' },
                { type: 'info', text: '--- The old rows correctly have NULL for the new column. Now, let\'s demonstrate Time Travel ---' },
                { type: 'command', text: "UPDATE my_first_table SET data = 'hello world (updated)' WHERE id = 1;" },
                { type: 'response', text: 'OK<br>Time taken: 0.9s' },
                { type: 'command', text: 'SELECT * FROM my_first_table;' },
                { type: 'response', text: '<pre class="text-xs whitespace-pre-wrap">+---+-----------------------+-----------------+\n|id |data                   |event_timestamp  |\n+---+-----------------------+-----------------+\n|1  |hello world (updated)  |null             |\n|2  |hello iceberg          |null             |\n+---+-----------------------+-----------------+</pre>Fetched 2 row(s)' },
                { type: 'command', text: "SELECT * FROM my_first_table TIMESTAMP AS OF '2025-07-21 10:00:00';" },
                { type: 'response', text: '<pre class="text-xs whitespace-pre-wrap">+---+-------------+-----------------+\n|id |data         |event_timestamp  |\n+---+-------------+-----------------+\n|1  |hello world  |null             |\n|2  |hello iceberg|null             |\n+---+-------------+-----------------+</pre>Fetched 2 row(s)' },
                { type: 'info', text: '--- Success! We traveled back in time to see the data before the update. Demo complete! ---' },
            ];

            function appendToTerminal(entry) {
                const div = document.createElement('div');
                if (entry.type === 'command') {
                    div.innerHTML = `<span class="text-cyan-400">spark-sql></span> <span class="text-white">${entry.text}</span>`;
                } else if (entry.type === 'response') {
                    div.innerHTML = `<div class="text-slate-300 mt-1 mb-3">${entry.text}</div>`;
                } else if (entry.type === 'info') {
                    div.innerHTML = `<div class="text-yellow-400 font-bold my-3 p-2 bg-yellow-900/50 rounded">${entry.text}</div>`;
                }
                terminal.appendChild(div);
                terminal.scrollTop = terminal.scrollHeight;
            }

            runDemoBtn.addEventListener('click', () => {
                if (demoStep < demoScript.length) {
                    appendToTerminal(demoScript[demoStep]);
                    demoStep++;
                }
                if (demoStep >= demoScript.length) {
                    runDemoBtn.textContent = 'Demo Finished';
                    runDemoBtn.disabled = true;
                    runDemoBtn.classList.remove('bg-sky-600', 'hover:bg-sky-500');
                    runDemoBtn.classList.add('bg-slate-500', 'cursor-not-allowed');
                }
            });
        });
    </script>
</body>
</html>
